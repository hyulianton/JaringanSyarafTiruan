{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVSH2FUbUEC1N8tU1/hMUb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyulianton/JaringanSyarafTiruan/blob/main/jst_Implementasi_Multilayer_Perceptron_(MLP)_Menggunakan_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proyek ini mendemonstrasikan bagaimana membangun dan melatih Jaringan Saraf Tiruan (Neural Network) dua lapisan menggunakan _framework_ **PyTorch**. PyTorch sangat efisien karena menggunakan **Automatic Differentiation (Autograd)** untuk menangani proses Backpropagation yang rumit secara otomatis, sehingga kita bisa fokus pada arsitektur dan pelatihan.\n",
        "\n",
        "## **Bagian 0: Setup dan Mendapatkan Data**\n",
        "\n",
        "Kita akan menggunakan `sklearn` untuk memuat dan memproses data, dan `torch` untuk mendefinisikan model dan melatihnya.\n",
        "\n",
        "### **Sel 0.1: Impor Library**\n"
      ],
      "metadata": {
        "id": "AeSwfYJfleYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nkAIyPL9kxHL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sel 0.2: Mendownload, Memuat, dan Preprocessing Data**\n",
        "\n",
        "Kita memuat _dataset_ Iris, melakukan _scaling_, dan mengubahnya menjadi _tensor_ PyTorch.\n"
      ],
      "metadata": {
        "id": "EhNXEpjilsHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Muat Dataset Iris (Download dilakukan oleh load_iris)\n",
        "iris = load_iris()\n",
        "X = iris.data   # Fitur (4 kolom)\n",
        "y = iris.target # Label (0, 1, 2)\n",
        "\n",
        "# 2. Scaling Fitur\n",
        "# Penting agar gradien descent bekerja dengan baik\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# 3. Membagi Data\n",
        "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Konversi ke PyTorch Tensor\n",
        "# Fitur (X) harus float, Label (y) harus Long/Int untuk CrossEntropyLoss\n",
        "X_train = torch.FloatTensor(X_train_np)\n",
        "X_test = torch.FloatTensor(X_test_np)\n",
        "y_train = torch.LongTensor(y_train_np)\n",
        "y_test = torch.LongTensor(y_test_np)\n",
        "\n",
        "print(f\"Dimensi Data Pelatihan (Tensor): {X_train.shape}\") # torch.Size([120, 4])\n",
        "print(f\"Dimensi Label Pelatihan (Tensor): {y_train.shape}\")  # torch.Size([120])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xezl8X_slvta",
        "outputId": "73aa9242-d03d-49cb-c0ad-a802505035f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensi Data Pelatihan (Tensor): torch.Size([120, 4])\n",
            "Dimensi Label Pelatihan (Tensor): torch.Size([120])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan:** Dalam PyTorch, label untuk _CrossEntropyLoss_ (klasifikasi multi-kelas) harus berupa bilangan bulat tunggal (LongTensor: 0, 1, 2) BUKAN _One-Hot Encoding_.\n",
        "\n",
        "### **Sel 0.3: Data Loading (Batching)**\n",
        "\n",
        "Kita menggunakan `DataLoader` untuk membagi data menjadi _Mini-Batch_, yang penting untuk efisiensi pelatihan.\n"
      ],
      "metadata": {
        "id": "L-KCFvRSl-Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gabungkan fitur dan label menjadi dataset\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "\n",
        "# Buat DataLoader untuk Mini-Batch\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(f\"Jumlah Mini-Batch per Epoch: {len(train_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVrdFejGmCoj",
        "outputId": "3ffb5657-603f-47ba-9581-34842169deec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah Mini-Batch per Epoch: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan:** `DataLoader` akan mengiterasi data dalam potongan 16 sampel, secara otomatis mengacak (_shuffle_) data di setiap _epoch_.\n",
        "\n",
        "----------\n",
        "\n",
        "## **Bagian 1: Mendefinisikan Arsitektur MLP**\n",
        "\n",
        "Kita mendefinisikan model MLP menggunakan kelas `nn.Module` PyTorch.\n",
        "\n",
        "### **Sel 1.1: Kelas MLP 2-Lapisan**\n"
      ],
      "metadata": {
        "id": "alhFUp5SmNrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Arsitektur Jaringan\n",
        "input_size = 4\n",
        "hidden_size = 10\n",
        "output_size = 3 # 3 kelas Iris\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "\n",
        "        # Lapisan Tersembunyi (Linear + ReLU)\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Lapisan Output (Linear)\n",
        "        # Catatan: Softmax diterapkan secara implisit di Loss Function\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Implementasi Forward Pass\"\"\"\n",
        "        out = self.fc1(x) # Z1 = X @ W1 + b1\n",
        "        out = self.relu(out) # A1 = ReLU(Z1)\n",
        "        out = self.fc2(out) # Z2 = A1 @ W2 + b2 (Output logit)\n",
        "        return out\n",
        "\n",
        "# Inisialisasi Model\n",
        "model = SimpleMLP(input_size, hidden_size, output_size)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sppnVshpmRB2",
        "outputId": "5f37a51d-4f91-4e06-d747-7d758c5ac534"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleMLP(\n",
            "  (fc1): Linear(in_features=4, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (fc2): Linear(in_features=10, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan:**\n",
        "\n",
        "-   `nn.Linear` secara otomatis menginisialisasi bobot (W) dan bias (b).\n",
        "    \n",
        "-   Metode `forward` mendefinisikan alur komputasi: X→FC1→ReLU→FC2→Output.\n",
        "    \n",
        "\n",
        "### **Sel 1.2: Definisi Loss dan Optimizer**\n",
        "\n",
        "Kita tentukan \"hakim kesalahan\" (_Loss_) dan \"panduan menuruni lembah\" (_Optimizer_)."
      ],
      "metadata": {
        "id": "p8YGfgkOmzBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Loss Function (Fungsi Rugi)\n",
        "# CrossEntropyLoss menggabungkan Softmax dan Negative Log Likelihood Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 2. Optimizer (Gradient Descent / Adam)\n",
        "# Adam adalah versi canggih dari Gradient Descent\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "print(f\"Loss Function: {criterion}\")\n",
        "print(f\"Optimizer: {optimizer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4e_0LyQm3y3",
        "outputId": "6c93905f-c69c-4cd9-ac54-c21d06f720ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Function: CrossEntropyLoss()\n",
            "Optimizer: Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan:** `optim.Adam` akan secara otomatis menerapkan pembaruan parameter (Wbaru​=Wlama​−η⋅∂W∂L​) menggunakan gradien yang dihitung.\n",
        "\n",
        "----------\n",
        "\n",
        "## **Bagian 2: Pelatihan (Training Loop)**\n",
        "\n",
        "Kita menjalankan siklus pelatihan (Forward → Loss → Backward → Update) untuk beberapa _epoch_.\n",
        "\n",
        "### **Sel 2.1: Siklus Pelatihan**\n"
      ],
      "metadata": {
        "id": "dBC3C9fenDqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "\n",
        "        # --- 1. Forward Pass ---\n",
        "        outputs = model(X_batch)\n",
        "\n",
        "        # --- 2. Hitung Loss ---\n",
        "        loss = criterion(outputs, y_batch)\n",
        "\n",
        "        # --- 3. Backward Pass ---\n",
        "        # Mengatur semua gradien menjadi nol sebelum menghitung yang baru\n",
        "        optimizer.zero_grad()\n",
        "        # PyTorch menghitung dL/dW untuk semua bobot secara otomatis\n",
        "        loss.backward()\n",
        "\n",
        "        # --- 4. Update Parameter ---\n",
        "        # Menerapkan Gradient Descent (menggunakan gradien yang baru dihitung)\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOJAKDZnnIbj",
        "outputId": "03de3027-4c33-43e4-8b73-ea0e1064ede8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 0.0023\n",
            "Epoch [100/500], Loss: 0.0143\n",
            "Epoch [150/500], Loss: 0.0488\n",
            "Epoch [200/500], Loss: 0.0008\n",
            "Epoch [250/500], Loss: 0.1893\n",
            "Epoch [300/500], Loss: 0.0002\n",
            "Epoch [350/500], Loss: 0.0205\n",
            "Epoch [400/500], Loss: 0.0139\n",
            "Epoch [450/500], Loss: 0.0418\n",
            "Epoch [500/500], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Penjelasan:** Perhatikan tiga baris ajaib PyTorch:\n",
        "\n",
        "1.  `optimizer.zero_grad()`: Harus dipanggil di awal setiap iterasi.\n",
        "    \n",
        "2.  `loss.backward()`: **Backpropagation otomatis!** PyTorch menghitung semua ∂W∂L​.\n",
        "    \n",
        "3.  `optimizer.step()`: Menerapkan pembaruan Wbaru​=Wlama​−… menggunakan gradien yang baru dihitung.\n",
        "    \n",
        "\n",
        "----------\n",
        "\n",
        "## **Bagian 3: Evaluasi Model**\n",
        "\n",
        "Kita mengukur kinerja model pada data uji.\n",
        "\n",
        "### **Sel 3.1: Menghitung Akurasi**"
      ],
      "metadata": {
        "id": "PPVVyKJXnQCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matikan mode training (tidak perlu menghitung gradien)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # 1. Forward Pass pada data uji\n",
        "    outputs = model(X_test)\n",
        "\n",
        "    # 2. Ambil kelas prediksi (indeks dengan skor tertinggi)\n",
        "    # torch.max(outputs, 1) mengembalikan nilai max dan indeksnya\n",
        "    _, predicted_labels = torch.max(outputs.data, 1)\n",
        "\n",
        "    # 3. Hitung Akurasi\n",
        "    total = y_test.size(0)\n",
        "    correct = (predicted_labels == y_test).sum().item()\n",
        "    accuracy = correct / total\n",
        "\n",
        "print(f\"\\n--- HASIL AKHIR ---\")\n",
        "print(f\"Total Sampel Uji: {total}\")\n",
        "print(f\"Prediksi Benar: {correct}\")\n",
        "print(f\"Akurasi Model pada Data Uji: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4494n6PnU2N",
        "outputId": "e65a97b9-a2d8-4bdf-ac83-c5e095bec32f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- HASIL AKHIR ---\n",
            "Total Sampel Uji: 30\n",
            "Prediksi Benar: 30\n",
            "Akurasi Model pada Data Uji: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kesimpulan:** Anda telah berhasil melatih MLP di PyTorch! Proses pelatihan jauh lebih ringkas dan fokus pada **arsitektur** dan **aliran data**, karena PyTorch menangani kalkulus **Backpropagation** di balik layar, memungkinkan efisiensi tinggi—khususnya ketika berhadapan dengan **Big Data** dan jaringan yang sangat dalam.\n"
      ],
      "metadata": {
        "id": "D85RVlMsnhpi"
      }
    }
  ]
}